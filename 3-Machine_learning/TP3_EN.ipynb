{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF 8215 - Intelligence artif.: méthodes et algorithmes \n",
    "## Fall 2018 - TP3 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due date: December 6**\n",
    "\n",
    "**Files to submit:**\n",
    "    * TP3_EN.ipynb filled\n",
    "    * SoftmaxClassifier.py filled\n",
    "    * test_prediction.csv prediction file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab is to give you an overview of the general course of a machine learning project while familiarizing you with adapted python libraries.\n",
    "\n",
    "\n",
    "In the first part, you will implement a multi-class classification algorithm called **softmax regression** using only **numpy** library and embed it in the **scikit-learn** library.\n",
    "\n",
    "In the second part, you will learn about the **dataset** used for this project. Moreover, you will have to perform the **preprocessing** of these data so that it can be used in conventional machine learning algorithms. To this end, you will use **pandas** and **scikit-learn** libraries.\n",
    "\n",
    "Finally, in the third part, you will compare the efficiency of the model that you have implemented with other models already implemented in **scikit-learn**. Then you will try to improve the performance of the selected algorithm.\n",
    "\n",
    "Once all these steps are done, you will submit your results on the **kaggle** platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install **pandas** and **scikit-learn**, the easiest way is to download and install **Anaconda**, which groups together the most used packages for scientific computing and data science.\n",
    "\n",
    "You will find the distribution here: https://www.anaconda.com/download/#linux.\n",
    "\n",
    "Make sure you have **scikit-learn** **20.0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Competition (2 points)\n",
    "\n",
    "When you finish the lab, you can submit your predictions on **kaggle**, you will get your performance in terms of **log loss**.\n",
    "You can then send me this result by email (laurent.boucaud@polymtl.ca) and join your prediction file on the test set (for verification).\n",
    "\n",
    "A conversation in the forum will be created to keep up to date the best score obtained by one of the teams of the course.\n",
    "\n",
    "As long as no forum is created, **do not send me your performances if they are above 0.8 of log loss**.\n",
    "\n",
    "Once the first best score posted in the forum, **only give me your results if your log loss is lower than the previous best score**.\n",
    "\n",
    "The number of points obtained will be proportional to the ranking of the teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Softmax Regression (10 points)\n",
    "\n",
    "\n",
    "In this part you will implement **softmax regression**, the **logistic regression** variant which allows you to perform classification for a class number greater than 2.\n",
    "\n",
    "The code to be completed is in the **SoftmaxClassifier.py** file.\n",
    "\n",
    "**For this exercise, the constraint is to use only the numpy library **\n",
    "\n",
    "## Sklearn encapsulation\n",
    "\n",
    "\n",
    "The class **SoftmaxClassifier** inherits from the **BaseEstimator** and **ClassifierMixin** classes from **scikit-learn** which will allow us to easily use the tools provided by scikit-learn with our classifier later.\n",
    "\n",
    "For compatibility, the classifier necessarily implements the methods:\n",
    "\n",
    "* **fit**: responsible for training the model\n",
    "* **predict_proba**: Predicts the probability of each class for each example in the dataset provided.\n",
    "* **predict**: Predicts the class for each example in the provided dataset.\n",
    "* **score**: quantifies the difference between the predicted classes and the actual classes for the dataset provided\n",
    "\n",
    "\n",
    "## Train/Test set:\n",
    "\n",
    "When one wants to test the performance of learning a machine learning algorithm, one **does not test it on the data used for learning**.\n",
    "\n",
    "Indeed, what interests us is that our algorithm is able to generalize its predictions to the data that it has never seen.\n",
    "\n",
    "To illustrate, if we test an algorithm on the training data, we test its ability to **learn by heart** the dataset and not to **generalize**.\n",
    "\n",
    "Therefore, when receiving a new dataset, the first thing to do is to **split it into two parts**: a **train set** (**70-80%** of the dataset) and a **test set** (**20-30%** of the dataset).\n",
    "\n",
    "All **data processing** and **learning algorithms** should be learned only on the training set and then applied to the test set.\n",
    "\n",
    "By doing so, the lack of prior knowledge of the test set during training is ensured.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Gradient descent is an algorithm that allows finding the optimal solution of a certain number of problems. The principle is as follows: we define a **cost function J** that characterizes the problem.\n",
    "This function depends on a set of **$\\theta$** parameters. Gradient descent seeks to **minimize** the cost function by **iteratively modifying** the parameters.\n",
    "\n",
    "### Gradient\n",
    "\n",
    "The cost function gradient for a given $\\theta$, is the direction in which $\\theta$ must be modified to reduce the value of the cost function.\n",
    "\n",
    "The cost function is minimal when the gradient is zero.\n",
    "\n",
    "Concretely, we initialize $\\theta$ randomly, and we do at each iteration a step to reduce the cost function until convergence of the algorithm to a minimum of the cost function.\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "\n",
    "The learning rate represents the size of the step that will be made in the direction of the gradient.\n",
    "The larger it is, the faster the convergence, but there is a risk that the algorithm will diverge.\n",
    "\n",
    "The smaller it is, the slower the convergence.\n",
    "\n",
    "### Batch gradient descent\n",
    "\n",
    "There are several gradient descent algorithms. We will use Batch gradient descent.\n",
    "\n",
    "In this algorithm, before updating $\\theta$, we calculate the gradients on all the training examples.\n",
    "\n",
    "### Epoch\n",
    "\n",
    "This is a step of the gradient descent, a single gradient update.\n",
    "\n",
    "### Bias/Variance tradeoff\n",
    "\n",
    "When training a machine learning algorithm we look for a tradeoff between **bias** and **variance**.\n",
    "\n",
    "A model with a **strong bias**, is a model that is **too simple** for the given data structure (e.g., a linear model for quadratic data), this limits the capacity of the model to generalize. We also call bias  **underfitting**.\n",
    "\n",
    "A model with a **high variance** means that it is sensitive to small variations in training data, this corresponds to **overfitting**, i.e., the model is too close to the structure of the training set which **limits its ability to generalize**.\n",
    "\n",
    "A model with a **significant bias** will have a **poor** performance over the **training set**.\n",
    "A model with a **significant variance** will have a much worse **performance** on the entire **test set** than on the  **train set**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "\n",
    "In machine learning to represent a vector of categorical data, we use one-hot encoding.\n",
    "\n",
    "For a vector containing 5 examples and 3 different categories, it is represented as a matrix of size 5 by 3. This matrix is entirely filled with 0 except for the index corresponding to the number of the class for each example.\n",
    "\n",
    "\n",
    "For example\n",
    "$ y = \\left(\\begin{array}{cc} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "2 \\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "becomes:\n",
    "\n",
    "$ yohe =  \\left(\\begin{array}{cc} \n",
    "1. & 0. & 0.\\\\\n",
    "1. & 0. & 0.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "0. & 0. & 1.\\\\\n",
    "0. & 1. & 0.\\\\\n",
    "\\end{array}\\right) $\n",
    "\n",
    "\n",
    "#### Question 1 (1 point)\n",
    "Implement the **_one_hot** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight matrix\n",
    "\n",
    "Let $ X_{m * n} $ be the example matrix and $ \\Theta _{n*K} $ the weight matrix with:\n",
    "\n",
    "* **m** number of examples\n",
    "* **n** number of features\n",
    "* **k** number of target classes\n",
    "\n",
    "\n",
    "\n",
    "It is common to add an additional column to X, this column is filled with 1. To take into account this change, we must add a line to the matrix $\\Theta$.\n",
    "\n",
    "We get X_bias$_{m*(n+1)}$ et $ \\Theta _{(n+1)*K} $\n",
    "\n",
    "\n",
    "Intuitively, each class K is associated with a $\\theta$ column.\n",
    "\n",
    "We denote by $\\theta_k$ (n+1 dimension vector) the weight column associated with the prediction of class k .\n",
    "\n",
    "$\\Theta$ = [$\\theta_0$,$\\theta_1$... $\\theta_k$ ... $\\theta_n$ ]\n",
    "\n",
    "Thus $ z = x * \\Theta $ gives a vector of dimension K which are **logits** associated with x for each class.\n",
    "\n",
    "#### Question 2 (1 point)\n",
    "\n",
    "In the **fit** function in SoftmaxClassifier.py instantiate X_bias and initialize $\\Theta$  randomly. (line 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We want to convert the logit vector **z** obtained in the previous part into a **probability vector**.\n",
    "\n",
    "For this we define the **softmax function**:\n",
    "\n",
    "\n",
    "$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n",
    "\n",
    "\n",
    "Intuitively, for a logit of z, $z_k$, we take the exponential of this value and divide it by the sum of the exponentials of each logit of the vector **z**. We get $\\hat{p_x}^k$ the probability that the example **x** belongs to the class **k**.\n",
    "\n",
    "The operation is repeated for each logit of the vector **z**.\n",
    "\n",
    "We thus obtain a probability vector $\\hat{p_x}$ for an example **x**.\n",
    "\n",
    "The division makes it possible to make the sum of the terms of the vector $\\hat{p_x}$ equal to 1 which is indispensable for probabilities.\n",
    "\n",
    "#### Question 3 (1 point)\n",
    "Implement  **_softmax** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (1 point)\n",
    "\n",
    "Using the **_ softmax** function of question 3, implement the **predict_proba** and **predict** methods in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de coût Log loss\n",
    "\n",
    "Let log loss (ou cross entropy) be the cost function:\n",
    "\n",
    "$$ J( \\Theta) = \\frac{-1}{m}\\sum_{\\substack{1<i<m}} \\sum_{\\substack{1<k<K}} y_k^i log( \\hat{p_k}^i ) $$\n",
    "\n",
    "with:\n",
    "* **K** number of classes\n",
    "* **m** number of examples\n",
    "* $ \\hat{p_k}^i  $  probability that example i be of target class k\n",
    "* $y_k^i$ is 1 if the target class of example i is k, 0 otherwise\n",
    "\n",
    "**Implementation detail:** Cost function is not defined for probabilities taking values 0. or 1., we must ensure that given $\\epsilon$, probabilities are in  [$\\epsilon$, 1. - $\\epsilon$].\n",
    "#### Question 5 (1 point)\n",
    "\n",
    "Implement the **_ cost_function** method in SoftmaxClassifier.py by taking into account the **implementation detail** (self.eps variable) and use it to calculate the **loss** variable in the **fit** method (line 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function gradient\n",
    "\n",
    "The **gradient of J** with respect to $\\theta_k$ is :\n",
    "\n",
    "\n",
    "$$ \\Delta_{\\theta_k}J( \\Theta) = \\frac{1}{m} \\sum_{\\substack{1<i<m}}( \\hat{p_k}^i - y_k^i)x^i  $$\n",
    "\n",
    "with:\n",
    "* **K** number of target classes\n",
    "* **m** number of examples\n",
    "* $ \\hat{p_k}^i  $  probability that example i is of class k\n",
    "* $y_k^i$ is 1 if example i target class is k, 0 otherwise\n",
    "\n",
    "\n",
    "We can rewrite it as matrices, the **gradient of J** with respect to $\\Theta$** is :\n",
    "$$ \\Delta_J( \\Theta) = \\frac{1}{m} X_{bias}^T *( \\hat{p} - y_{ohe}) $$\n",
    "\n",
    "with:\n",
    "\n",
    "* $\\hat{p}$ predicted probability matrix for every example and every class\n",
    "* $y_{ohe}$ one-hot encoded y\n",
    "* $X_{bias}^T$  Transposed matrix of $X_{bias}$\n",
    "* **\\*** Dot product\n",
    "\n",
    "#### Question 6 (1 point)\n",
    "Implement  **_get_gradient** method in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights update\n",
    "\n",
    "When the gradient has been computed, we must update the weights with these gradients.\n",
    "\n",
    "\n",
    "$$ \\Theta  = \\Theta - \\gamma \\Delta J( \\Theta) $$\n",
    "\n",
    "\n",
    "with:\n",
    "* $\\Theta$ weight matrix\n",
    "* $\\gamma$  learning rate\n",
    "* $\\Delta J( \\Theta)$ gradient of $J( \\Theta)$ with respect to $\\Theta$\n",
    "\n",
    "#### Question 7 (1 point)\n",
    "Update **self.theta_** in the **fit** method in SoftmaxClassifier.py (line 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "To limit **overfitting**, we use the regularization, we add a term to the function of cost $J( \\Theta)$.\n",
    "\n",
    "This term will add constraints on the weight of the model during training.\n",
    "We will use the **L2** regularization:\n",
    "\n",
    "\n",
    "$$ L2(\\Theta) = \\alpha \\sum_{\\substack{1<=i<n}} \\sum_{\\substack{0<=k<K}} \\theta_{i,k}^2 $$ \n",
    "\n",
    "with:\n",
    "\n",
    "* $\\alpha$ regularization coefficient\n",
    "\n",
    "**Note:** The first sum does not start at 0 but at 1 because we do not adjust the weights associated with the X bias column.\n",
    "\n",
    "Adding this term leads the model to learn the data while keeping its weight as small as possible.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 8 (1 point)\n",
    "\n",
    "Modify the methods **_ get_gradient** and **_ cost_function** to take into account the regularization when the boolean self.regularization is true in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (1 point)\n",
    "\n",
    "The regularization term is used only during training. When one wants to evaluate the performance of the model **after training**, one uses the **non-regulated** cost function.\n",
    "\n",
    "Implement the **score** function that evaluates the quality of the prediction **after training** in SoftmaxClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Too many **epochs** can result in **overfitting**.\n",
    "To overcome this problem, we can use the mechanism of **early stopping**.\n",
    "This aims to stop the training if the difference in the cost function between two **consecutive epochs** is less than a defined **threshold**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 10 (1 point)\n",
    "\n",
    "Finish implementing the **fit** function by adding the **early stopping** mechanism when the **self.early_stopping** boolean is true. The threshold is given by the **self.threshold variable** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the solution:\n",
    "\n",
    "The code below imports the **iris** multiclass  classification dataset available on sklearn. The data is divided into two parts, the training set and the test set, and then they are normalized.\n",
    "\n",
    "The classifier implemented in the **SoftmaxClassifier.py** file is imported and then trained on the training set and tested on the test set.\n",
    "\n",
    "The purpose of this part is just to check your implementation **when you are sure your code is working**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load dataset\n",
    "data,target =load_iris().data,load_iris().target\n",
    "\n",
    "# split data in train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# standardize columns using normal distribution\n",
    "# fit on X_train and not on X_test to avoid Data Leakage\n",
    "s = StandardScaler()\n",
    "X_train = s.fit_transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "\n",
    "# import the custom classifier\n",
    "cl = SoftmaxClassifier()\n",
    "\n",
    "# train on X_train and not on X_test to avoid overfitting\n",
    "train_p = cl.fit_predict(X_train,y_train)\n",
    "test_p = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get relatively close values for the test and training set, and they are at least greater than 0.8, your model should be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (0.8706896551724137, 0.865546218487395, 0.8640202702702702, None)\n",
      "test : (0.8888888888888888, 0.8222222222222223, 0.8121212121212121, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# display precision, recall and f1-score on train/test set\n",
    "print(\"train : \"+ str(precision_recall_fscore_support(y_train, train_p,average = \"macro\")))\n",
    "print(\"test : \"+ str(precision_recall_fscore_support(y_test, test_p,average = \"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cl.losses_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing (8 points)\n",
    "\n",
    "##  Kaggle \n",
    "\n",
    "Kaggle is a website dedicated to machine learning. There is a large number of datasets.\n",
    "Competitions are organized by companies and organisations. These provide a dataset and a goal. The \"kagglers\" who participate in these competitions submit their results online. There are often prices or jobs for those who get the best results.\n",
    "\n",
    "This is a good way to develop machine learning skills on real datasets.\n",
    "\n",
    "You can create an account if you want to compare your results to those already online for the dataset we are going to study.\n",
    "\n",
    "You can create an account here: https://www.kaggle.com/\n",
    "\n",
    "## Austin Animal Center Shelter Animal Outcomes dataset\n",
    "The dataset that we will use is the \"Animal Outcomes dataset\" available at the following address: https://www.kaggle.com/c/shelter-animal-outcomes.\n",
    "\n",
    "This is a problem of **multi-class classification** where animals are collected in a shelter after being abandoned, the purpose is to predict how they will \"leave\" the place:\n",
    "* Adoption\n",
    "* Back to the owner\n",
    "* Death\n",
    "* Euthanasia\n",
    "* Transfer to another center\n",
    "\n",
    "For more information on data, go to kaggle.\n",
    "\n",
    "## Structure of a machine learning project\n",
    "\n",
    "The goal of the this part of the lab is to make you study a simplified version of a complete machine learning project:\n",
    "\n",
    "1. Data cleaning, missing value processing\n",
    "2. Formatting Data for Use in Machine Learning Algorithms\n",
    "3. Feature engineering transformation or feature combinations between them\n",
    "4. Comparison of the performances of the different choices made during the data processing\n",
    "5. Comparison of the performances of different models (including the one implemented in the first part)\n",
    "6. Optimization of hyper-parameters\n",
    "\n",
    "\n",
    "## Scikit-learn\n",
    "http://scikit-learn.org/stable/\n",
    "\n",
    "It is a machine learning and data mining library, it offers tools for data analysis and processing, classical machine learning algorithms such as neural networks, logistic regression, SVM or other, finally tools to compare models between them such as cross validation.\n",
    "\n",
    "## Pandas\n",
    "\n",
    "A library to store and manipulate data easily\n",
    "\n",
    "The two basic elements of pandas are the dataframe and the series.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.html\n",
    "\n",
    "## Data processing tutorial\n",
    "\n",
    "** Before continuing the lab **, familiarize yourself with the **pre-processing data**, **pandas** and **scikit-learn**, a tutorial is available in the file: *data_processing_tutorial.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### Load train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"data/\"\n",
    "X_train = pd.read_csv(PATH + \"train.csv\")\n",
    "X_test = pd.read_csv(PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useless features removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"OutcomeSubtype\",\"AnimalID\"])\n",
    "X_test = X_test.drop(columns = [\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.drop(columns = [\"OutcomeType\"]),X_train[\"OutcomeType\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hambone</td>\n",
       "      <td>2014-02-12 18:22:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "      <td>Brown/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emily</td>\n",
       "      <td>2013-10-13 12:44:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Cream Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pearce</td>\n",
       "      <td>2015-01-31 12:28:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "      <td>Blue/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-11 19:09:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Blue Cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-11-15 12:52:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "      <td>Tan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0  Hambone  2014-02-12 18:22:00        Dog  Neutered Male         1 year   \n",
       "1    Emily  2013-10-13 12:44:00        Cat  Spayed Female         1 year   \n",
       "2   Pearce  2015-01-31 12:28:00        Dog  Neutered Male        2 years   \n",
       "3      NaN  2014-07-11 19:09:00        Cat    Intact Male        3 weeks   \n",
       "4      NaN  2013-11-15 12:52:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                         Breed        Color  \n",
       "0        Shetland Sheepdog Mix  Brown/White  \n",
       "1       Domestic Shorthair Mix  Cream Tabby  \n",
       "2                 Pit Bull Mix   Blue/White  \n",
       "3       Domestic Shorthair Mix   Blue Cream  \n",
       "4  Lhasa Apso/Miniature Poodle          Tan  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer</td>\n",
       "      <td>2015-10-12 12:15:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>10 months</td>\n",
       "      <td>Labrador Retriever Mix</td>\n",
       "      <td>Red/White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>2014-07-26 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>2 years</td>\n",
       "      <td>German Shepherd/Siberian Husky</td>\n",
       "      <td>Black/Tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gus</td>\n",
       "      <td>2016-01-13 12:20:00</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Brown Tabby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pongo</td>\n",
       "      <td>2013-12-28 18:12:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>4 months</td>\n",
       "      <td>Collie Smooth Mix</td>\n",
       "      <td>Tricolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skooter</td>\n",
       "      <td>2015-09-24 17:59:00</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Miniature Poodle Mix</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name             DateTime AnimalType SexuponOutcome AgeuponOutcome  \\\n",
       "0    Summer  2015-10-12 12:15:00        Dog  Intact Female      10 months   \n",
       "1  Cheyenne  2014-07-26 17:59:00        Dog  Spayed Female        2 years   \n",
       "2       Gus  2016-01-13 12:20:00        Cat  Neutered Male         1 year   \n",
       "3     Pongo  2013-12-28 18:12:00        Dog    Intact Male       4 months   \n",
       "4   Skooter  2015-09-24 17:59:00        Dog  Neutered Male        2 years   \n",
       "\n",
       "                            Breed        Color  \n",
       "0          Labrador Retriever Mix    Red/White  \n",
       "1  German Shepherd/Siberian Husky    Black/Tan  \n",
       "2          Domestic Shorthair Mix  Brown Tabby  \n",
       "3               Collie Smooth Mix     Tricolor  \n",
       "4            Miniature Poodle Mix        White  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 5  examples of the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Return_to_owner\n",
       "1         Euthanasia\n",
       "2           Adoption\n",
       "3           Transfer\n",
       "4           Transfer\n",
       "Name: OutcomeType, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "To save you time, some of the columns (Name, DateTime, color) have already been processed.\n",
    "\n",
    "\n",
    "Using the tutorial provided, you must write a complete transformation pipeline for each of the remaining columns in the dataset (AgeuponOutcome, AnimalType, SexuponOutcome, Breed).\n",
    "\n",
    "You are **free** of your choices, but you must **justify** column by column.\n",
    "For example, you can choose to combine columns with each other, separate a column or eliminate a column completely if you correctly justify it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The already preprocessed part of the dataset is loaded in **X_train1** and **X_test1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.read_csv(\"data/train_preprocessed.csv\")\n",
    "X_test1 = pd.read_csv(\"data/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>HasName</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.421532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.471381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.868974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Color  HasName  Month  Day  Hour\n",
       "0  0.973624      1.0    2.0  1.0   3.0\n",
       "1 -1.421532      1.0   10.0  1.0   2.0\n",
       "2  0.973624      1.0    1.0  3.0   2.0\n",
       "3 -1.471381      0.0    7.0  1.0   3.0\n",
       "4 -0.868974      0.0   11.0  1.0   2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset part you have to process is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = [\"Color\",\"Name\",\"DateTime\"])\n",
    "X_test = X_test.drop(columns = [\"Color\",\"Name\",\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnimalType</th>\n",
       "      <th>SexuponOutcome</th>\n",
       "      <th>AgeuponOutcome</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Shetland Sheepdog Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>1 year</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Pit Bull Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>3 weeks</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Lhasa Apso/Miniature Poodle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AnimalType SexuponOutcome AgeuponOutcome                        Breed\n",
       "0        Dog  Neutered Male         1 year        Shetland Sheepdog Mix\n",
       "1        Cat  Spayed Female         1 year       Domestic Shorthair Mix\n",
       "2        Dog  Neutered Male        2 years                 Pit Bull Mix\n",
       "3        Cat    Intact Male        3 weeks       Domestic Shorthair Mix\n",
       "4        Dog  Neutered Male        2 years  Lhasa Apso/Miniature Poodle"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 11: AgeuponOutcome (1 point)\n",
    "\n",
    "Afin d'avoir des grandeurs comparables, les données ont été parcourues afin d'être converties en l'unité la plus petite, c'est-à-dire les jours. Les données sont ensuite normalisées avec le StandardScaler de sklearn afin de réduire l'overflow, ainsi que pour avoir des grandeurs comparables entre les attributs du modèle. Enfin, pour les données inconnues, elles ont d'abord été remplacées par un Not A Number (NaN) dans le parser, puis remplacées par la moyenne afin de ne pas introduire de biais dans les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12: AnimalType (1 point)\n",
    "\n",
    "Un encodeur one-hot a été utilisé avec les trois catégories suivantes: chat, chien, inconnu. Cette classification permet de facilement classer les animaux, tout en tenant compte de potentielles données qui n'auraient pas leur place dans ce classificateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13: SexuponOutcome (1 point)\n",
    "\n",
    "Cette catégorie a été divisée en 2 sous catégories: le sexe (mâle, femelle) et l'état de reproduction (intact ou infertile). Ces sous-catégories ont ensuite été encodées avec un encodeur one-hot. On réduit ainsi le nombre total de catégories (colonnes) dans notre classificateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14: Breed (1 point)\n",
    "\n",
    "Cette catégorie a été la plus complexe à traiter. Nous avons divisé la catégorie en 2 parties: la race, et le mix(oui/non). Ceci permet encore une fois de réduire grandement le nombre total de colonnes. Dans le cas des races séparées par le séparateur '/', elles sont considérées comme des \"mix\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "**Question 15: Fill the pipeline below (4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import TransformationWrapper\n",
    "from preprocessing import LabelEncoderP\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Custom functions for parsing (pipelines)\n",
    "def parse_reproduction_state(text):\n",
    "    if text == 'Unknown':\n",
    "        reproduction_state = 'Unknown'\n",
    "    else:\n",
    "        reproduction_state, _ = text.split(' ')\n",
    "    return reproduction_state\n",
    "\n",
    "\n",
    "def parse_sex(text):\n",
    "    if text == 'Unknown':\n",
    "        sex = 'Unknown'\n",
    "    else:\n",
    "        _, sex = text.split(' ')\n",
    "    return sex\n",
    "\n",
    "\n",
    "def parse_age(text):\n",
    "    if isinstance(text, str) and text[0].isdigit():\n",
    "        nbr, period = text.split(' ')\n",
    "        nbr = int(nbr)\n",
    "        if period[0] == 'y' or period[0] == 'Y':\n",
    "            temps = nbr * 365\n",
    "        elif period[0] == 'm' or period[0] == 'M':\n",
    "            temps = nbr * 30\n",
    "        elif period[0] == 'w' or period[0] == 'W':\n",
    "            temps = nbr * 7\n",
    "        elif period[0] == 'd' or period[0] == 'D':\n",
    "            temps = nbr\n",
    "        else:\n",
    "            temps = np.nan\n",
    "    else:\n",
    "        temps = np.nan\n",
    "\n",
    "    return temps\n",
    "\n",
    "\n",
    "def parse_breed(text):\n",
    "    text = text.replace(' ', '')\n",
    "    text = text.upper()\n",
    "    return text.replace('MIX', '')\n",
    "\n",
    "\n",
    "def parse_mix(text):\n",
    "    text = text.upper()\n",
    "    if (text.find('MIX') >= 0) or (text.find('/') >= 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Cat/Dog\n",
    "pipeline_animal_type = Pipeline([('type', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "                                 ('encode', LabelEncoderP()),\n",
    "                                 ('onehot', OneHotEncoder(categories='auto', sparse = False))])\n",
    "\n",
    "# Breed\n",
    "pipeline_mix = Pipeline([('mix', TransformationWrapper(transformation= parse_mix)),\n",
    "                         ('encode', LabelEncoderP())])\n",
    "\n",
    "pipeline_breed = Pipeline([('breed', TransformationWrapper(transformation= parse_breed)),\n",
    "                           ('encode', LabelEncoderP())])\n",
    "\n",
    "pipeline_breed_mix = Pipeline([('mix_and_breed', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "                               ('feats', FeatureUnion([\n",
    "                                   ('mix', pipeline_mix),\n",
    "                                   ('breed', pipeline_breed)\n",
    "                               ])),\n",
    "                               ('onehot', OneHotEncoder(categories='auto', sparse=False))])\n",
    "\n",
    "\n",
    "# Sex and reproduction\n",
    "pipeline_reproduction = Pipeline([('reproduction', TransformationWrapper(transformation = parse_reproduction_state)),\n",
    "                                  (\"encode\", LabelEncoderP())])\n",
    "\n",
    "pipeline_sex = Pipeline([('sex', TransformationWrapper(transformation = parse_sex)),\n",
    "                         (\"encode\", LabelEncoderP())])\n",
    "\n",
    "\n",
    "pipeline_sex_and_reproduction = Pipeline([(\"sex_and_reproduction\", SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "                                          ('feats', FeatureUnion([\n",
    "                                              ('reproduction', pipeline_reproduction),\n",
    "                                              ('sex', pipeline_sex)\n",
    "                                          ])),\n",
    "                                          (\"onehot\", OneHotEncoder(categories='auto', sparse=False))])\n",
    "\n",
    "# Age\n",
    "pipeline_age = Pipeline([('age', TransformationWrapper(transformation=parse_age)),\n",
    "                         ('age_imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "                         ('normalizer', StandardScaler())])\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = ColumnTransformer([('type', pipeline_animal_type, ['AnimalType']),\n",
    "                                   ('breed', pipeline_breed_mix, ['Breed']),\n",
    "                                   ('sex_and_reproduction', pipeline_sex_and_reproduction, ['SexuponOutcome']),\n",
    "                                   ('age', pipeline_age, ['AgeuponOutcome'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# column_names = []\n",
    "X_train_prepared = pd.DataFrame(full_pipeline.fit_transform(X_train))\n",
    "X_test_prepared = pd.DataFrame(full_pipeline.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate both part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train1,X_train_prepared], axis = 1)\n",
    "X_test = pd.concat([X_test1,X_test_prepared], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model selection (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the target class as integers to use it\n",
    "with scikit-learn algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_label = LabelEncoder()\n",
    "y_train_label = target_label.fit_transform(y_train)\n",
    "print(target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set\n",
    "\n",
    "To compare different models with each other, we can not use the test set, otherwise one would be tempted to keep the model corresponding best to the test set which could lead to overfitting.\n",
    "\n",
    "It is common to create a new set of the size of the test set, the  **validation** set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Cross-validation is a useful method for comparing the performance of different machine learning models **without creating a validation set**.\n",
    "\n",
    "There are different types of cross-validation, the most classic procedure is:\n",
    "* Randomly divide the training set into two parts (90% / 10% for example).\n",
    "* Train the model on biggest part, and test it on the other part.\n",
    "* Repeat n times\n",
    "* Calculate the mean and standard deviation of the results\n",
    "\n",
    "The benefits are:\n",
    "* Consider the entire training set for the evaluation (without ignoring the data we would have use in the validation set)\n",
    "* Obtaining the standard deviation of the results allows a better evaluation of the model's accuracy.\n",
    "\n",
    "The main disadvantage is the computation time, since one carries out the learning of the model several times, this method can be impossible for datasets containing a large number of example (> 10e5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: StratifiedKFold (1 point)\n",
    "\n",
    "By observing the class distribution of the target attribute (using the pandas visualization functions), justify the use of the sklearn **StratifiedKFold** object for division of the training set when doing cross-validation instead of a pure **random** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode du \"StratifiedKFold\" permet d'avoir des sous-ensembles de données d'entrainement et de test qui sont plus semblables entre eux que ceux qui seraient obtenues par une méthode complètement aléatoire. Les \"folds\" sont alors tous similaires, alors qu'avec une méthode aléatoire, par exemple, on pourrait avoir toutes les données du 1er fold avec la même classe, toutes celles du 2e fold avec la 2e classe, etc. Notre entraînement serait alors médiocre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16: (1 point)\n",
    "\n",
    "\n",
    "**Choose at least two models allowing the multiclass classification on sklearn in addition to the model implemented in the first part of the TP**.\n",
    "\n",
    "**Complete the compare function that performs the crossvalidation for different models and different metrics, and returns the list of averages and standard deviations for each of the metrics, for each of the models.**\n",
    "\n",
    "**Based on the different metrics, conclude on the best performing model.**\n",
    "\n",
    "Evaluate the models for the different metrics proposed:\n",
    "* **log loss**: this is the kaggle evaluation metric for this dataset\n",
    "* **precision**: corresponds to the quality of the prediction, the number of classes correctly predicted by the total prediction number\n",
    "* **recall**: the number of elements belonging to a class, identified as such, divided by the total number of elements of that class.\n",
    "* **f-score**: an average of accuracy and recall\n",
    "\n",
    "**Note: Precision and recall are two complementary measures for evaluating a multi-class classification model.**\n",
    "\n",
    "In the case of a binary classification with an important target class imbalance, (90% / 10%), evaluating the classification result with accuracy (number of correct predictions divided by the total number of predictions), a very good score (90% accuracy) can be obtained by choosing to systematically predict the majority class.\n",
    "\n",
    "In such a case, the precision would be high in the same way, but the recall would be very low, indicating the mediocrity of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(models, X_train, y_train_label, nb_runs, scoring):\n",
    "    losses = []\n",
    "    for model in models:\n",
    "        cv_results = cross_validate(model, X_train, y_train_label, scoring=scoring, cv=nb_runs, return_train_score=True)\n",
    "        losses.append([cv_results['train_neg_log_loss'], cv_results['train_precision_macro'], cv_results['train_recall_macro'], cv_results['train_f1_macro']])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores: \n",
      "[array([-0.91870472,  0.48065943,  0.36835906,  0.36390616]), array([-0.03084399,  0.97027678,  0.97970698,  0.97470543]), array([-31.07221581,   0.40956542,   0.306429  ,   0.1217525 ])]\n",
      "Standard deviation of scores: \n",
      "[array([0.00346299, 0.00597385, 0.00120838, 0.00150361]), array([0.00129329, 0.00279935, 0.00077728, 0.00157531]), array([0.05310417, 0.00568312, 0.00087458, 0.00173356])]\n"
     ]
    }
   ],
   "source": [
    "from SoftmaxClassifier import SoftmaxClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "nb_run = 3\n",
    "\n",
    "#Models to compare\n",
    "models = [SoftmaxClassifier(),\n",
    "          DecisionTreeClassifier(random_state=0),\n",
    "          GaussianNB()\n",
    "         ]\n",
    "\n",
    "scoring = ['neg_log_loss', 'precision_macro','recall_macro','f1_macro']\n",
    "\n",
    "scores = compare(models, X_train, y_train_label, nb_run, scoring)\n",
    "\n",
    "scores_mean = []\n",
    "scores_std = []\n",
    "for i in range(0, len(models)):\n",
    "    scores_mean.append(np.mean(scores[i], axis=1))\n",
    "    scores_std.append(np.std(scores[i], axis=1))\n",
    "\n",
    "print('Average scores: ')\n",
    "print(scores_mean)\n",
    "print('Standard deviation of scores: ')\n",
    "print(scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit tout de suite en regardant les différents scores obtenus lors de la validation croisée que l'arbre de décision (DecisionTreeClassifier) est de loin le meilleur modèle prédictif. En effet, celui-ci atteint une précision et un recall très élevés, soit au-dessus de 97%, ce qui donne un score f1 au-dessus de 97% aussi. La performance des autres modèles est plutôt médiocre. Le SoftmaxClassifier n'atteint même pas une précision de 50%, avec un recall et un score f1 de maigres 36%. La GaussianNB qui suppose naïvement une indépendance entre les différents attributs de X est encore pire, avec un recall exécrable de 28%. On voit aussi que la métrique de perte logarithmique (valeur absolue de neg_log_loss) abonde dans le même sens, avec une très petite - donc excellente - valeur d'environ 0,0313 pour l'arbre de décision, un mauvais 0,918 pour le softmax et finalement un énorme 32,211 pour la gaussienne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17: Confusion matrix (0.5 point)\n",
    "\n",
    "The confusion matrix A is such that $A_{i,j}$ represents the number of examples of class i classified as belonging to class j.\n",
    "\n",
    "Train the selected model on the entire training set.\n",
    "Using the confusion matrix and class distribution, analyze in more detail the performance of the chosen model and justify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected model\n",
    "\n",
    "selected_model = DecisionTreeClassifier(random_state=0)\n",
    "my_model = selected_model.fit(X_train, y_train_label)\n",
    "y_pred = my_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adoption</th>\n",
       "      <th>Died</th>\n",
       "      <th>Euthanasia</th>\n",
       "      <th>Return_to_owner</th>\n",
       "      <th>Transfer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adoption</th>\n",
       "      <td>10705</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Died</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Euthanasia</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>1502</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Return_to_owner</th>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4603</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transfer</th>\n",
       "      <td>242</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>9057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Adoption  Died  Euthanasia  Return_to_owner  Transfer\n",
       "Adoption            10705     0           3               16        45\n",
       "Died                    1   192           0                1         3\n",
       "Euthanasia             31     3        1502                3        16\n",
       "Return_to_owner       155     1          15             4603        12\n",
       "Transfer              242    12          47               64      9057"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_train_label, y_pred), columns = target_label.classes_, index = target_label.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Target class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adoption' 'Died' 'Euthanasia' 'Return_to_owner' 'Transfer']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(target_label.classes_)\n",
    "pd.Series(y_train_label).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle sélectionné est celui par arbre de décision (DecisionTreeClassifier). En observant la matrice de confusion obtenue pour ce modèle, on observe d'abord de bons résultats au niveau du classement des essais. Effectivement, c'est sur la diagonale de la matrice que l'on retrouve les plus gros nombres (représentant le nombre d'essais), ce qui signifie qu'ils ont été correctement identifiés. \n",
    "\n",
    "Il est également intéressant de regarder les proportions d'éléments mal classés pour chacune des catégories. 429 des 670 essais mal classés ont été identifiés dans la catégorie \"Adoption\", c'est-à-dire 64% des mauvaises identifications, ou 1,6% des essais totaux. 16 des 670 essais mal classés ont été identifiés dans la catégorie \"Mort\", soit 2,34% des mauvaises identifications, ou 0,06% des essais totaux. 65 des 670 essais mal classés ont été identifiés dans la catégorie \"Euthanasie\", soit 9,7% des mauvaises identifications, ou 0,24% des essais totaux. 84 des 670 essais mal classés ont été identifiés dans la catégorie \"Retour au propriétaire\", soit 12,54% des mauvaises identifications, ou 0,31% des essais totaux. 76 des 670 essais mal classés ont été identifiés dans la catégorie \"Transfert\", soit 11,34% des mauvaises identifications, ou 0,28% des essais totaux.\n",
    "\n",
    "Or, on sait également que 40,29% des données devraient être classées en \"Adoption\", 0,74% en \"Mort\", 5,82% en \"Euthanasie\", 17,91% en \"Retour au propritétaire\", et 35,25% en \"Transfert\" si l'algorithme était parfait.\n",
    "\n",
    "Il est alors possible de constater que l'algorithme performe généralement très bien, mais fait un peu plus d'erreurs pour certaines catégories. Par exemple, on pourrait s'attendre à ce que la proportion d'erreur pour chaque classe soit semblable à la distribution réelle des classes. Or, la proportion d'erreur pour la classe \"Adoption\" est plus grande que la proportion réelle de données qui appartienne à cette classe. L'algorithme a alors tendance à mal classer des données dans cette catégorie plus que dans les autres catégories. Ce problème peut être expliqué par le traitement de données effectué. L'arbre de décision effectue des décisions à la chaîne, un feature à la fois, avec les features qu'il considère comme les plus importants en premier. Or, si certains features considérés importants \"favorisent\" le classement dans \"Adoption\", on retrouvera alors une plus grande proportion d'éléments faussement classés dans cette catégorie.\n",
    "\n",
    "La performance du DecisionTreeClassifier demeure toutefois excellente. Cela peut être expliquée par la manière dont les données ont été traitées. Effectivement, la majorité du traitement de données consiste à faire du one-hot encoding plutôt que de calculer des valeurs décimales complexes. Or, ce type d'information d'entrée est plus appropriée pour une arbre de décision, car ce-dernier effectue un choix à chaque étage de l'arbre, c'est-à-dire pour chaque feature. Or, il est plus facile d'effectuer un choix sur une valeur binaire que sur une valeur numérique. C'est aussi une opération beaucoup plus rapide, ce qui explique l'excellent temps d'exécution de cet algorithme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3: Hyper-parameters optimization (1 point)\n",
    "\n",
    "Hyper-parameters are the parameters set before the learning phase. To optimize the performance of the model, we can select the best hyper-parameters.\n",
    "\n",
    "Using sklearn, optimize the hyper-parameters of the model you have selected and show that the performance has been improved.\n",
    "For example, you can use: **GridSearchCV**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make the prediction on the test set and give your results when submitting the lab.\n",
    "\n",
    "**Optional**: You can submit your results on kaggle and note your performance in terms of log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous a été impossible de trouver des paramètres permettant d'améliorer la performance du DecisionTreeClassifier. Toutefois, un exemple du code à utiliser afin d'optimiser les paramètres a été ajouté en commentaire à la fin du document \"main.py\" pour référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
